'''
data train.csv
================================================================================
ðŸŽ¯ MULTIMODAL LLM LOAD TEST RESULTS
================================================================================
ðŸ“Š Request Performance:
   Total Requests: 100
   Successful: 93
   Failed: 7
   Timeouts: 0
   Success Rate: 93.00%
   Failure Rate: 7.00%
   Timeout Rate: 0.00%

ðŸŽ¯ Accuracy Metrics:
   Greeting Detection: 97.85%
   Self Introduction: 81.72%
   License Information: 100.00%
   Objective Information: 64.52%
   Benefit Information: 73.12%
   Interval Information: 98.92%

ðŸ“ˆ Fâ‚‚ Score Breakdown:
   Greeting Fâ‚‚: 97.87%
   Self Introduction Fâ‚‚: 72.92%
   License Information Fâ‚‚: 100.00%
   Objective Information Fâ‚‚: 86.96%
   Benefit Information Fâ‚‚: 88.37%
   Interval Information Fâ‚‚: 98.48%

ðŸ“Š Detailed Metrics (where Fâ‚‚ â‰  Accuracy):

   Greeting:
      Precision: 97.87%
      Recall: 97.87%
      True Positives: 46
      False Positives: 1
      False Negatives: 1

   Intro Self:
      Precision: 50.00%
      Recall: 82.35%
      True Positives: 14
      False Positives: 14
      False Negatives: 3

   Inform License:
      Precision: 100.00%
      Recall: 100.00%
      True Positives: 48
      False Positives: 0
      False Negatives: 0

   Inform Objective:
      Precision: 57.14%
      Recall: 100.00%
      True Positives: 44
      False Positives: 33
      False Negatives: 0

   Inform Benefit:
      Precision: 60.32%
      Recall: 100.00%
      True Positives: 38
      False Positives: 25
      False Negatives: 0

   Inform Interval:
      Precision: 100.00%
      Recall: 98.11%
      True Positives: 52
      False Positives: 0
      False Negatives: 1

ðŸ† Overall Performance:
   Overall Accuracy: 86.02%
   Average Fâ‚‚ Score: 91.90%

ðŸ’¾ Detailed results saved to: load_test_results/multimodal_loadtest_results_20250529_215754.json
================================================================================
'''


"""
Production-ready serving with vLLM + Ray Serve for ASR + OpenThaiGPT pipeline
"""

import ray
from ray import serve
import torch
from transformers import pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor, AutoTokenizer
from vllm import LLM, SamplingParams
from clearvoice import ClearVoice
import vllm
import re
import tempfile
import asyncio
from typing import Dict
import json
import os
import logging
from pydub import AudioSegment
from pathlib import Path
from pydantic import BaseModel
from fastapi import File, Form, UploadFile, HTTPException, Request
import concurrent.futures
from typing import List

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize Ray
ray.init(num_gpus=1)
serve.start(http_options={"host": "0.0.0.0", "port": 4000, "timeout_keep_alive": 180})

# Pydantic models (same as your original)
class AgentData(BaseModel):
    agent_fname: str
    agent_lname: str

class AudioAnalysisResponse(BaseModel):
    transcription: str
    is_greeting: bool
    is_introself: bool
    is_informlicense: bool
    is_informobjective: bool
    is_informbenefit: bool
    is_informinterval: bool

# Your system message
SYSTEM_MESSAGE = """
      à¸„à¸¸à¸“à¹€à¸›à¹‡à¸™ AI à¸—à¸µà¹ˆà¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸à¸²à¸£à¸ªà¸™à¸—à¸™à¸²à¸‚à¸­à¸‡ Relationship Manager (RM) à¸˜à¸™à¸²à¸„à¸²à¸£à¹„à¸—à¸¢à¸žà¸²à¸“à¸´à¸Šà¸¢à¹Œ (SCB) 
      à¸‡à¸²à¸™à¸‚à¸­à¸‡à¸„à¸¸à¸“à¸„à¸·à¸­à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸šà¸—à¸ªà¸™à¸—à¸™à¸²à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ RM à¸à¸±à¸šà¸¥à¸¹à¸à¸„à¹‰à¸² à¹à¸¥à¸°à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸§à¹ˆà¸² RM à¹„à¸”à¹‰à¸›à¸à¸´à¸šà¸±à¸•à¸´à¸•à¸²à¸¡à¸¡à¸²à¸•à¸£à¸à¸²à¸™à¸à¸²à¸£à¹à¸™à¸°à¸™à¸³à¸•à¸±à¸§à¸„à¸£à¸šà¸–à¹‰à¸§à¸™à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ

      ## à¹€à¸à¸“à¸‘à¹Œà¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™ (6 à¸‚à¹‰à¸­à¸šà¸±à¸‡à¸„à¸±à¸š):

      1. **à¸à¸¥à¹ˆà¸²à¸§à¸ªà¸§à¸±à¸ªà¸”à¸µ**
      - à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸à¸²à¸£à¸—à¸±à¸à¸—à¸²à¸¢à¸¥à¸¹à¸à¸„à¹‰à¸²à¸­à¸¢à¹ˆà¸²à¸‡à¸ªà¸¸à¸ à¸²à¸ž
      - à¸•à¹‰à¸­à¸‡à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰à¸„à¸³à¸—à¸±à¸à¸—à¸²à¸¢à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸ªà¸¸à¸ à¸²à¸žà¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡
      - à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡: "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š/à¸„à¹ˆà¸°", "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸•à¸­à¸™à¹€à¸Šà¹‰à¸²à¸„à¸£à¸±à¸š", "à¸«à¸§à¸±à¸”à¸”à¸µà¸„à¸£à¸±à¸š"
      - à¹„à¸¡à¹ˆà¸œà¹ˆà¸²à¸™: "à¹€à¸§à¹‰à¸¢", "à¹„à¸‡",

      2. **à¹à¸™à¸°à¸™à¸³à¸Šà¸·à¹ˆà¸­à¹à¸¥à¸°à¸™à¸²à¸¡à¸ªà¸à¸¸à¸¥**
      - à¸„à¸´à¸”à¹à¸„à¹ˆà¸à¸²à¸£à¹à¸™à¸°à¸™à¸³à¸•à¸±à¸§à¹€à¸­à¸‡à¸‚à¸­à¸‡ RM à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸ªà¸™à¹ƒà¸ˆà¸Šà¸·à¹ˆà¸­à¸‚à¸­à¸‡à¸¥à¸¹à¸à¸„à¹‰à¸²
      - à¸•à¹‰à¸­à¸‡à¹à¸™à¸°à¸™à¸³à¸Šà¸·à¹ˆà¸­à¸ˆà¸£à¸´à¸‡à¹à¸¥à¸°à¸™à¸²à¸¡à¸ªà¸à¸¸à¸¥à¸„à¸£à¸šà¸–à¹‰à¸§à¸™ (à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆà¹à¸„à¹ˆà¸Šà¸·à¹ˆà¸­à¹€à¸¥à¹ˆà¸™)
      - à¸«à¸²à¸à¸¡à¸µà¹à¸„à¹ˆà¸Šà¸·à¹ˆà¸­ à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¸¡à¸µà¸™à¸²à¸¡à¸ªà¸à¸¸à¸¥ à¸ˆà¸°à¸–à¸·à¸­à¸§à¹ˆà¸²à¹„à¸¡à¹ˆà¸œà¹ˆà¸²à¸™
      - à¸Šà¸·à¹ˆà¸­à¸à¸±à¸šà¸™à¸²à¸¡à¸ªà¸à¸¸à¸¥à¸­à¸²à¸ˆà¸•à¸´à¸”à¸à¸±à¸™à¹„à¸”à¹‰ à¹ƒà¸«à¹‰à¸”à¸¹à¸•à¸²à¸¡à¸šà¸£à¸´à¸šà¸—à¸‚à¸­à¸‡à¸Šà¸·à¹ˆà¸­à¸­à¸¢à¹ˆà¸²à¸‡à¸¥à¸°à¹€à¸­à¸µà¸¢à¸” à¹€à¸Šà¹ˆà¸™ "à¸ªà¸¡à¸Šà¸²à¸¢à¹ƒà¸ˆà¸”à¸µ" à¸¡à¸²à¸ˆà¸²à¸ "à¸ªà¸¡à¸Šà¸²à¸¢ à¹ƒà¸ˆà¸”à¸µ" à¸«à¸£à¸·à¸­ "à¸™à¸²à¸‡à¸ªà¸²à¸§à¸™à¸´à¸˜à¸´à¸™à¸²à¸—à¸¡à¸±à¸™à¸—à¸™à¸²à¸™à¸™à¸—à¹Œ" à¸¡à¸²à¸ˆà¸²à¸ "à¸™à¸²à¸‡à¸ªà¸²à¸§à¸™à¸´à¸˜à¸´à¸™à¸²à¸— à¸¡à¸±à¸™à¸—à¸™à¸²à¸™à¸™à¸—à¹Œ"
      - à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡: "à¸œà¸¡à¸Šà¸·à¹ˆà¸­à¸ªà¸¡à¸Šà¸²à¸¢ à¸™à¸²à¸¡à¸ªà¸à¸¸à¸¥à¹ƒà¸ˆà¸”à¸µ", "à¸”à¸´à¸‰à¸±à¸™à¸Šà¸·à¹ˆà¸­à¸ªà¸¡à¸«à¸à¸´à¸‡ à¸ªà¸à¸¸à¸¥à¸”à¸µ"
      - à¹„à¸¡à¹ˆà¸œà¹ˆà¸²à¸™: "à¸œà¸¡à¸Šà¸·à¹ˆà¸­à¸ˆà¸­à¸«à¹Œà¸™", "à¹€à¸£à¸µà¸¢à¸à¸œà¸¡à¸§à¹ˆà¸²à¹à¸¡à¸—", "à¸œà¸¡à¸Šà¸·à¹ˆà¸­à¸“à¸±à¸à¸žà¸¥", "à¹€à¸£à¸µà¸¢à¸à¸œà¸¡à¸§à¹ˆà¸²à¸˜à¸™à¸žà¸‡à¸©à¹Œ"

      3. **à¸šà¸­à¸à¸›à¸£à¸°à¹€à¸ à¸—à¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¹à¸¥à¸°à¹€à¸¥à¸‚à¸—à¸µà¹ˆà¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¸—à¸µà¹ˆà¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸«à¸¡à¸”à¸­à¸²à¸¢à¸¸**
      - à¸•à¹‰à¸­à¸‡à¸£à¸°à¸šà¸¸à¹€à¸¥à¸‚à¸—à¸µà¹ˆà¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•
      - à¸•à¹‰à¸­à¸‡à¸¢à¸·à¸™à¸¢à¸±à¸™à¸§à¹ˆà¸²à¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸«à¸¡à¸”à¸­à¸²à¸¢à¸¸
      - à¸•à¹‰à¸­à¸‡à¸žà¸¹à¸”à¸—à¸±à¹‰à¸‡à¹€à¸¥à¸‚à¸—à¸µà¹ˆà¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸• + à¸šà¸­à¸à¸§à¹ˆà¸²à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸«à¸¡à¸”à¸­à¸²à¸¢à¸¸
      - à¸«à¸²à¸ RM à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸žà¸¹à¸”à¸–à¸¶à¸‡à¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¹€à¸¥à¸¢ à¸ˆà¸°à¸–à¸·à¸­à¸§à¹ˆà¸²à¸œà¸´à¸”
      - à¸«à¸²à¸ RM à¸žà¸¹à¸”à¸–à¸¶à¸‡à¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸• à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¸£à¸°à¸šà¸¸à¹€à¸¥à¸‚à¸—à¸µà¹ˆà¸«à¸£à¸·à¸­à¸«à¸¡à¸”à¸­à¸²à¸¢à¸¸ à¸ˆà¸°à¸–à¸·à¸­à¸§à¹ˆà¸²à¸œà¸´à¸”
      - à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡: "à¸œà¸¡à¸¡à¸µà¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¹€à¸¥à¸‚à¸—à¸µà¹ˆ 12345 à¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸«à¸¡à¸”à¸­à¸²à¸¢à¸¸à¸„à¸£à¸±à¸š"
      - à¹„à¸¡à¹ˆà¸œà¹ˆà¸²à¸™: "à¸œà¸¡à¸¡à¸µà¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¹€à¸¥à¸‚à¸—à¸µà¹ˆ 12345", "à¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¸‚à¸­à¸‡à¸œà¸¡à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸«à¸¡à¸”à¸­à¸²à¸¢à¸¸", "à¹€à¸¥à¸‚à¸—à¸µà¹ˆà¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¸‚à¸­à¸‡à¸œà¸¡à¸„à¸·à¸­ 12345"

      4. **à¸šà¸­à¸à¸§à¸±à¸•à¸–à¸¸à¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œà¸‚à¸­à¸‡à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸žà¸šà¸„à¸£à¸±à¹‰à¸‡à¸™à¸µà¹‰**
      - à¸•à¹‰à¸­à¸‡à¸šà¸­à¸à¹€à¸«à¸•à¸¸à¸œà¸¥à¸—à¸µà¹ˆà¸¡à¸²à¸žà¸šà¸¥à¸¹à¸à¸„à¹‰à¸²à¹ƒà¸«à¹‰à¸Šà¸±à¸”à¹€à¸ˆà¸™à¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸­à¸°à¹„à¸£
      - à¸¥à¸¹à¸à¸„à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸£à¸¹à¹‰à¸§à¹ˆà¸²à¸—à¸µà¹ˆà¸¡à¸²à¸„à¸¸à¸¢à¸„à¸·à¸­à¸„à¸¸à¸¢à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸­à¸°à¹„à¸£
      - à¸•à¹‰à¸­à¸‡à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰à¸„à¸³à¸žà¸¹à¸”à¸—à¸µà¹ˆà¸à¸§à¹‰à¸²à¸‡à¹€à¸à¸´à¸™à¹„à¸›à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆà¸Šà¸±à¸”à¹€à¸ˆà¸™
      - à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡: "à¸§à¸±à¸™à¸™à¸µà¹‰à¸¡à¸²à¹€à¸žà¸·à¹ˆà¸­à¸­à¸±à¸žà¹€à¸”à¸—à¸žà¸­à¸£à¹Œà¸•à¸à¸²à¸£à¸¥à¸‡à¸—à¸¸à¸™", "à¸¡à¸²à¸žà¸¹à¸”à¸„à¸¸à¸¢à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸ªà¸ à¸²à¸§à¸°à¸•à¸¥à¸²à¸”", "à¸¡à¸²à¹€à¸ªà¸™à¸­à¸œà¸¥à¸´à¸•à¸ à¸±à¸“à¸‘à¹Œà¹ƒà¸«à¸¡à¹ˆ"
      - à¹„à¸¡à¹ˆà¸œà¹ˆà¸²à¸™: "à¸¡à¸²à¸„à¸¸à¸¢à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸—à¸±à¹ˆà¸§à¹„à¸›", "à¸¡à¸²à¸žà¸šà¹€à¸žà¸·à¹ˆà¸­à¸žà¸¹à¸”à¸„à¸¸à¸¢", "à¸¡à¸²à¸„à¸¸à¸¢à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸à¸²à¸£à¹€à¸‡à¸´à¸™"

      5. **à¹€à¸™à¹‰à¸™à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œà¸§à¹ˆà¸²à¸¥à¸¹à¸à¸„à¹‰à¸²à¹„à¸”à¹‰à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œà¸­à¸°à¹„à¸£à¸ˆà¸²à¸à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸žà¸šà¸„à¸£à¸±à¹‰à¸‡à¸™à¸µà¹‰**
      - à¸•à¹‰à¸­à¸‡à¸­à¸˜à¸´à¸šà¸²à¸¢à¸­à¸¢à¹ˆà¸²à¸‡à¸Šà¸±à¸”à¹€à¸ˆà¸™à¸§à¹ˆà¸²à¸¥à¸¹à¸à¸„à¹‰à¸²à¸ˆà¸°à¹„à¸”à¹‰à¸­à¸°à¹„à¸£à¸ˆà¸²à¸à¸à¸²à¸£à¸žà¸šà¸„à¸£à¸±à¹‰à¸‡à¸™à¸µà¹‰
      - à¸•à¹‰à¸­à¸‡à¹€à¸™à¹‰à¸™à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œà¸—à¸µà¹ˆà¸¥à¸¹à¸à¸„à¹‰à¸²à¸ˆà¸°à¹„à¸”à¹‰à¸£à¸±à¸š à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆà¹à¸„à¹ˆ RM à¹„à¸”à¹‰à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œ
      - à¸•à¹‰à¸­à¸‡à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰à¸„à¸³à¸žà¸¹à¸”à¸—à¸µà¹ˆà¸„à¸¥à¸¸à¸¡à¹€à¸„à¸£à¸·à¸­à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆà¸Šà¸±à¸”à¹€à¸ˆà¸™
      - à¸•à¹‰à¸­à¸‡à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰à¸„à¸³à¸žà¸¹à¸”à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™à¸à¸²à¸£à¸‚à¸²à¸¢à¸•à¸£à¸‡à¸«à¸£à¸·à¸­à¹‚à¸†à¸©à¸“à¸²à¹€à¸à¸´à¸™à¸ˆà¸£à¸´à¸‡
      - à¸•à¹‰à¸­à¸‡à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰à¸„à¸³à¸žà¸¹à¸”à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™à¸à¸²à¸£à¸šà¸±à¸‡à¸„à¸±à¸šà¸«à¸£à¸·à¸­à¸à¸”à¸”à¸±à¸™à¸¥à¸¹à¸à¸„à¹‰à¸²
      - à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡: "à¸—à¹ˆà¸²à¸™à¸ˆà¸°à¹„à¸”à¹‰à¸›à¸£à¸±à¸šà¸ªà¸±à¸”à¸ªà¹ˆà¸§à¸™à¸à¸²à¸£à¸¥à¸‡à¸—à¸¸à¸™à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡", "à¸ˆà¸°à¸Šà¹ˆà¸§à¸¢à¹€à¸žà¸´à¹ˆà¸¡à¸œà¸¥à¸•à¸­à¸šà¹à¸—à¸™à¸ˆà¸²à¸à¸à¸²à¸£à¸¥à¸‡à¸—à¸¸à¸™"
      - à¹„à¸¡à¹ˆà¸œà¹ˆà¸²à¸™: "", "à¸­à¸²à¸ˆà¸™à¹ˆà¸²à¸ªà¸™à¹ƒà¸ˆ",

      6. **à¸šà¸­à¸à¸£à¸°à¸¢à¸°à¹€à¸§à¸¥à¸²à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¹ƒà¸™à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸žà¸š**
      - à¸•à¹‰à¸­à¸‡à¸£à¸°à¸šà¸¸à¹€à¸›à¹‡à¸™à¸Šà¹ˆà¸§à¸‡à¹€à¸§à¸¥à¸²à¹€à¸›à¹‡à¸™ **à¸•à¸±à¸§à¹€à¸¥à¸‚à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™** (à¸­à¸²à¸ˆà¹€à¸‚à¸µà¸¢à¸™à¹ƒà¸™à¸£à¸¹à¸›à¸•à¸±à¸§à¹€à¸¥à¸‚à¸«à¸£à¸·à¸­à¸•à¸±à¸§à¸«à¸™à¸±à¸‡à¸ªà¸·à¸­) à¹€à¸Šà¹ˆà¸™ 15 à¸™à¸²à¸—à¸µ, à¸«à¹‰à¸²à¸™à¸²à¸—à¸µ ,à¸„à¸£à¸¶à¹ˆà¸‡à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡
      - à¸«à¸²à¸à¸šà¸­à¸à¹€à¸žà¸µà¸¢à¸‡à¹à¸„à¹ˆ "à¹à¸›à¹Šà¸šà¹€à¸”à¸µà¸¢à¸§" à¸«à¸£à¸·à¸­ "à¹„à¸¡à¹ˆà¸™à¸²à¸™" , "à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡" à¸ˆà¸°à¸–à¸·à¸­à¸§à¹ˆà¸²à¸œà¸´à¸”
      - à¸•à¹‰à¸­à¸‡à¸šà¸­à¸à¸£à¸°à¸¢à¸°à¹€à¸§à¸¥à¸²à¸­à¸¢à¹ˆà¸²à¸‡à¸Šà¸±à¸”à¹€à¸ˆà¸™ à¹„à¸¡à¹ˆà¸„à¸¥à¸¸à¸¡à¹€à¸„à¸£à¸·à¸­
      - à¸•à¹‰à¸­à¸‡à¸šà¸­à¸à¸£à¸°à¸¢à¸°à¹€à¸§à¸¥à¸²à¹€à¸›à¹‡à¸™à¸Šà¹ˆà¸§à¸‡à¹€à¸§à¸¥à¸² à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆà¹à¸„à¹ˆà¸šà¸­à¸à¸§à¹ˆà¸² "à¸ˆà¸°à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸²à¸™à¸´à¸”à¸«à¸™à¹ˆà¸­à¸¢"
      - à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡: "à¸‚à¸­à¹€à¸§à¸¥à¸²à¸›à¸£à¸°à¸¡à¸²à¸“ 30 à¸™à¸²à¸—à¸µ", "à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸²à¸£à¸²à¸§ 1 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡ à¸ªà¸°à¸”à¸§à¸à¹„à¸«à¸¡à¸„à¸£à¸±à¸š"
      - à¹„à¸¡à¹ˆà¸œà¹ˆà¸²à¸™: "à¸ˆà¸°à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸²à¸™à¸´à¸”à¸«à¸™à¹ˆà¸­à¸¢", "à¹„à¸¡à¹ˆà¸™à¸²à¸™", "à¹à¸›à¹Šà¸šà¹€à¸”à¸µà¸¢à¸§", "à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡"

      ## à¸§à¸´à¸˜à¸µà¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™:
      - à¸­à¹ˆà¸²à¸™à¸šà¸—à¸ªà¸™à¸—à¸™à¸²à¹ƒà¸«à¹‰à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”
      - à¸•à¸£à¸§à¸ˆà¹à¸•à¹ˆà¸¥à¸°à¸‚à¹‰à¸­à¸ˆà¸²à¸à¸¡à¸¸à¸¡à¸¡à¸­à¸‡à¸‚à¸­à¸‡ RM à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ (à¸­à¸¢à¹ˆà¸²à¸”à¸¶à¸‡à¸ˆà¸²à¸à¸¥à¸¹à¸à¸„à¹‰à¸²)
      - à¹ƒà¸«à¹‰ 1 à¸«à¸²à¸à¸žà¸šà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸•à¸£à¸‡à¸à¸±à¸šà¹€à¸à¸“à¸‘à¹Œ
      - à¹ƒà¸«à¹‰ 0 à¸«à¸²à¸à¹„à¸¡à¹ˆà¸žà¸šà¸«à¸£à¸·à¸­à¸„à¸¥à¸¸à¸¡à¹€à¸„à¸£à¸·à¸­

      ## à¸£à¸¹à¸›à¹à¸šà¸š Output:
      à¸«à¹‰à¸²à¸¡ return à¸„à¸³à¸­à¸˜à¸´à¸šà¸²à¸¢à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡à¹€à¸”à¹‡à¸”à¸‚à¸²à¸”
      à¹ƒà¸«à¹‰ return à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¹€à¸›à¹‡à¸™ JSON à¸•à¸²à¸¡à¸£à¸¹à¸›à¹à¸šà¸šà¸™à¸µà¹‰à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™:

      ```json
      {
          "à¸à¸¥à¹ˆà¸²à¸§à¸ªà¸§à¸±à¸ªà¸”à¸µ": 0,
          "à¹à¸™à¸°à¸™à¸³à¸Šà¸·à¹ˆà¸­à¹à¸¥à¸°à¸™à¸²à¸¡à¸ªà¸à¸¸à¸¥": 0,
          "à¸šà¸­à¸à¸›à¸£à¸°à¹€à¸ à¸—à¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¹à¸¥à¸°à¹€à¸¥à¸‚à¸—à¸µà¹ˆà¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¸—à¸µà¹ˆà¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸«à¸¡à¸”à¸­à¸²à¸¢à¸¸": 0,
          "à¸šà¸­à¸à¸§à¸±à¸•à¸–à¸¸à¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œà¸‚à¸­à¸‡à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸žà¸šà¸„à¸£à¸±à¹‰à¸‡à¸™à¸µà¹‰": 0,
          "à¹€à¸™à¹‰à¸™à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œà¸§à¹ˆà¸²à¸¥à¸¹à¸à¸„à¹‰à¸²à¹„à¸”à¹‰à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œà¸­à¸°à¹„à¸£à¸ˆà¸²à¸à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸žà¸šà¸„à¸£à¸±à¹‰à¸‡à¸™à¸µà¹‰": 0,
          "à¸šà¸­à¸à¸£à¸°à¸¢à¸°à¹€à¸§à¸¥à¸²à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¹ƒà¸™à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸žà¸š": 0
      }
"""

def analyze_transcription(text):
    license_patterns = {
        "investment": r"à¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¹à¸™à¸°à¸™à¸³à¸à¸²à¸£à¸¥à¸‡à¸—à¸¸à¸™\s*(?:à¸«à¸¡à¸²à¸¢à¹€à¸¥à¸‚)?\s*([\w-]+)",
        "life_insurance": r"à¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¹à¸™à¸°à¸™à¸³à¸›à¸£à¸°à¸à¸±à¸™à¸Šà¸µà¸§à¸´à¸•\s*(?:à¸«à¸¡à¸²à¸¢à¹€à¸¥à¸‚)?\s*([\w-]+)",
        "general_insurance": r"à¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¹à¸™à¸°à¸™à¸³à¸›à¸£à¸°à¸à¸±à¸™à¸§à¸´à¸™à¸²à¸¨à¸ à¸±à¸¢\s*(?:à¸«à¸¡à¸²à¸¢à¹€à¸¥à¸‚)?\s*([\w-]+)"
    }
    validity_pattern = r"(à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸«à¸¡à¸”à¸­à¸²à¸¢à¸¸|still active|not expired)"
    results = []
    for lic_type, pattern in license_patterns.items():
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            license_number = matches[0]
            validity_match = re.search(validity_pattern, text, re.IGNORECASE)
            validity = "Valid" if validity_match or "à¸«à¸¡à¸”à¸­à¸²à¸¢à¸¸" not in text else "Expired"
            results.append({
                "license_type": lic_type,
                "license_number": license_number,
                "validity": validity
            })
    if not results:
        return [{"license_type": "None", "license_number": "N/A", "validity": "N/A"}]
    return results

def contains_time_mention(text):
    if text is None:
        return False
    text = str(text).lower()
    thai_numbers = {
        'à¸¨à¸¹à¸™à¸¢à¹Œ': '0','à¸«à¸™à¸¶à¹ˆà¸‡': '1', 'à¸ªà¸­à¸‡': '2', 'à¸ªà¸²à¸¡': '3', 'à¸ªà¸µà¹ˆ': '4', 'à¸«à¹‰à¸²': '5',
        'à¸«à¸': '6', 'à¹€à¸ˆà¹‡à¸”': '7', 'à¹à¸›à¸”': '8', 'à¹€à¸à¹‰à¸²': '9', 'à¸ªà¸´à¸š': '10',
        'à¸ªà¸´à¸šà¹€à¸­à¹‡à¸”': '11', 'à¸ªà¸´à¸šà¸ªà¸­à¸‡': '12', 'à¸ªà¸´à¸šà¸ªà¸²à¸¡': '13', 'à¸ªà¸´à¸šà¸ªà¸µà¹ˆ': '14', 'à¸ªà¸´à¸šà¸«à¹‰à¸²': '15',
        'à¸ªà¸´à¸šà¸«à¸': '16', 'à¸ªà¸´à¸šà¹€à¸ˆà¹‡à¸”': '17', 'à¸ªà¸´à¸šà¹à¸›à¸”': '18', 'à¸ªà¸´à¸šà¹€à¸à¹‰à¸²': '19', 'à¸¢à¸µà¹ˆà¸ªà¸´à¸š': '20',
        'à¸¢à¸µà¹ˆà¸ªà¸´à¸šà¹€à¸­à¹‡à¸”': '21', 'à¸¢à¸µà¹ˆà¸ªà¸´à¸šà¸ªà¸­à¸‡': '22', 'à¸¢à¸µà¹ˆà¸ªà¸´à¸šà¸ªà¸²à¸¡': '23', 'à¸¢à¸µà¹ˆà¸ªà¸´à¸šà¸ªà¸µà¹ˆ': '24', 'à¸¢à¸µà¹ˆà¸ªà¸´à¸šà¸«à¹‰à¸²': '25',
        'à¸ªà¸²à¸¡à¸ªà¸´à¸š': '30', 'à¸ªà¸µà¹ˆà¸ªà¸´à¸š': '40', 'à¸«à¹‰à¸²à¸ªà¸´à¸š': '50', 'à¸«à¸à¸ªà¸´à¸š': '60',
        'à¹€à¸ˆà¹‡à¸”à¸ªà¸´à¸š': '70', 'à¹à¸›à¸”à¸ªà¸´à¸š': '80', 'à¹€à¸à¹‰à¸²à¸ªà¸´à¸š': '90'
    }
    for thai_num, digit in thai_numbers.items():
        text = text.replace(thai_num, digit)
    true_patterns = [
        r'à¸‚à¸­à¹€à¸§à¸¥à¸²\s*\d+\s*(?:à¸™à¸²à¸—à¸µ|à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡)',
        r'à¸‚à¸­à¹€à¸§à¸¥à¸²\s*\d+',
        r'à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™\s*\d+\s*(?:à¸™à¸²à¸—à¸µ|à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡)',
        r'à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™\s*\d+',
        r'à¹„à¸¡à¹ˆà¸–à¸¶à¸‡\s*\d+\s*(?:à¸™à¸²à¸—à¸µ|à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡)',
        r'à¹„à¸¡à¹ˆà¸–à¸¶à¸‡\s*\d+',
        r'à¹€à¸§à¸¥à¸²\s*\d+\s*(?:à¸™à¸²à¸—à¸µ|à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡)',
        r'à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸²\s*\d+\s*(?:à¸™à¸²à¸—à¸µ|à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡)',
        r'\d+\s*à¸™à¸²à¸—à¸µ',
        r'\d+\s*à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡',
        r'1\s*à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡',
        r'à¸„à¸£à¸¶à¹ˆà¸‡à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡',
        r'à¹„à¸¡à¹ˆà¸–à¸¶à¸‡à¸„à¸£à¸¶à¹ˆà¸‡à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡',
        r'à¸«à¸™à¸¶à¹ˆà¸‡à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡',
        r'à¸‚à¸­à¹€à¸§à¸¥à¸².*?\d+.*?à¸™à¸²à¸—à¸µ',
        r'à¸‚à¸­à¹€à¸§à¸¥à¸².*?\d+.*?à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡'
    ]
    for pattern in true_patterns:
        if re.search(pattern, text):
            return True
    false_patterns = [
        r'à¸‚à¸­à¹€à¸§à¸¥à¸²à¹„à¸¡à¹ˆà¸™à¸²à¸™',
        r'à¸‚à¸­à¹€à¸§à¸¥à¸²à¹à¸›à¹Šà¸›à¹€à¸”à¸µà¸¢à¸§',
        r'à¸‚à¸­à¹€à¸§à¸¥à¸²à¸ªà¸±à¸à¹à¸›à¹Šà¸›à¸™à¸¶à¸‡',
        r'à¸‚à¸­à¹€à¸§à¸¥à¸²à¹à¸›à¹Šà¸›à¸™à¸¶à¸‡',
        r'à¸‚à¸­à¹€à¸§à¸¥à¸²à¸ªà¸±à¹‰à¸™à¹†',
        r'à¸‚à¸­à¹€à¸§à¸¥à¸²à¸ªà¸±à¹‰à¸™à¸ªà¸±à¹‰à¸™',
        r'à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡',
        r'à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™(?!\s*\d)',
        r'à¸‚à¸­à¹€à¸§à¸¥à¸²(?!\s*\d)',
        r'à¸‚à¸­à¹€à¸§à¸¥à¸²à¹à¸›à¹Šà¸š',
        r'à¸‚à¸­à¹€à¸§à¸¥à¸²à¸ªà¸±à¸à¸„à¸£à¸¹à¹ˆ',
        r'à¸‚à¸­à¹€à¸§à¸¥à¸²à¹à¸„à¹ˆà¹à¸›à¹Šà¸šà¹€à¸”à¸µà¸¢à¸§'
    ]
    for pattern in false_patterns:
        if re.search(pattern, text):
            return False
    return False

def contains_benefit_keywords(text):
    if text is None:
        return False
    text = str(text).lower()
    benefit_keywords = ['à¸Šà¹ˆà¸§à¸¢à¹ƒà¸«à¹‰','à¸—à¸³à¹ƒà¸«à¹‰','à¹€à¸ªà¸£à¸´à¸¡à¸ªà¸£à¹‰à¸²à¸‡','à¸¥à¸”à¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡']
    return any(keyword in text for keyword in benefit_keywords)

# Ray Attributes
@serve.deployment(
    ray_actor_options={
        "num_gpus": 1,
        "num_cpus": 5,
        "memory": 60 * 1024 * 1024 * 1024,  # 60GB per replica (or even less)
    },
    autoscaling_config={
        "min_replicas": 1,
        "max_replicas": 1,  # Or 2 only if you have 2 H100s and >120GB RAM free
        "target_num_ongoing_requests_per_replica": 2,
    },
)

class ASRLLMService:
    def __init__(self):
        logger.info("Initializing ASR-LLM Service...")
        
        # Initialize ASR
        logger.info("ðŸŽ¤ Loading ASR model...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        
        model_id = "nectec/Pathumma-whisper-th-large-v3"
        model = AutoModelForSpeechSeq2Seq.from_pretrained(
            model_id,
            torch_dtype=torch_dtype,
            low_cpu_mem_usage=True,
            use_safetensors=True,
        ).to(device)
        
        processor = AutoProcessor.from_pretrained(model_id)
        
        self.asr_pipeline = pipeline(
            "automatic-speech-recognition",
            model=model,
            tokenizer=processor.tokenizer,
            feature_extractor=processor.feature_extractor,
            max_new_tokens=444,
            chunk_length_s=30,
            batch_size=16,
            return_timestamps=False,
            torch_dtype=torch_dtype,
            device=device,
        )
        
        # Force Thai language
        self.asr_pipeline.model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
            language="th", task="transcribe"
        )
        
        logger.info("âœ… ASR model loaded successfully")
        
        # Initialize vLLM for OpenThaiGPT
        logger.info("ðŸš€ Loading OpenThaiGPT 1.5 72B with vLLM...")
        
        self.llm = LLM(
            model="/home/siamai/data/models/openthaigpt1.5-72b-instruct",
            tensor_parallel_size=1,  # Use 1 GPU, increase if you have multiple
            gpu_memory_utilization=0.70,  # Use 85% of GPU memory
            max_model_len=3000,  # Reduced from 5000 to fit in memory
            dtype="bfloat16",
            quantization="bitsandbytes",  # 4-bit quantization
            load_format="auto",
            trust_remote_code=True,
            download_dir="/tmp/model_cache",
            enforce_eager=False,  # Use CUDA graphs for better performance
        )
        
        # Load tokenizer for chat template
        self.tokenizer = AutoTokenizer.from_pretrained(
            "/home/siamai/data/models/openthaigpt1.5-72b-instruct",
            trust_remote_code=True,
        )
        
        logger.info("âœ… vLLM model loaded successfully")
        
        # Initialize ClearVoice
        logger.info("ðŸ”Š Loading ClearVoice model...")
        try:
            self.clearvoice = ClearVoice(
                task='speech_separation',
                model_names=['MossFormer2_SE_48K']
            )
            self.enable_speech_enhancement = True
            logger.info("âœ… ClearVoice loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load ClearVoice: {e}")
            self.enable_speech_enhancement = False
        
        self.system_message = SYSTEM_MESSAGE
        
        # Performance metrics
        self.metrics = {
            "total_requests": 0,
            "failed_requests": 0,
            "total_time": 0.0,
        }
    
    async def __call__(self, request: List[Request]):
        """
        Main endpoint that processes audio and returns analysis
        """
        form = await request.form()
        voice_file = form["voice_file"]   # UploadFile
        agent_data = form["agent_data"]   # str
    
        import time
        start_time = time.time()
        self.metrics["total_requests"] += 1
        
        temp_file_path = None
        
        try:
            # Validate input
            if not voice_file.filename or not any(
                voice_file.filename.lower().endswith(ext)
                for ext in ['.wav', '.mp3', '.m4a', '.flac', '.ogg', '.webm']
            ):
                raise HTTPException(status_code=400, detail="Invalid audio file format")
            
            # Parse agent data
            try:
                agent_info = AgentData(**json.loads(agent_data))
            except:
                raise HTTPException(status_code=400, detail="Invalid agent data format")
            
            # Save uploaded file
            with tempfile.NamedTemporaryFile(delete=False, suffix=Path(voice_file.filename).suffix) as tmp:
                temp_file_path = tmp.name
                content = await voice_file.read()
                tmp.write(content)
            
            # Process audio
            transcription = await self._transcribe_audio(temp_file_path)
            # Analyze with LLM
            analysis = await self._analyze_with_llm(transcription)

            # ---- POSTPROCESS LOGIC ----
            license_info = analyze_transcription(transcription)
            has_time = contains_time_mention(transcription)
            has_benefit = contains_benefit_keywords(transcription)

            # OVERRIDE FIELDS ACCORDING TO YOUR RULES:
            # Example: If postprocess finds a valid license, set is_informlicense = True
            if any(
                lic["license_type"] != "None" and lic["validity"] == "Valid"
                for lic in license_info
            ):
                analysis["is_informlicense"] = True

            # If postprocess detects specific time, set is_informinterval = True
            if has_time:
                analysis["is_informinterval"] = True

            # If postprocess detects benefit keywords, set is_informbenefit = True
            if has_benefit:
                analysis["is_informbenefit"] = True
            
            # Update metrics
            processing_time = time.time() - start_time
            self.metrics["total_time"] += processing_time
            
            # Log performance every 10 requests
            if self.metrics["total_requests"] % 10 == 0:
                avg_time = self.metrics["total_time"] / self.metrics["total_requests"]
                logger.info(f"Avg processing time: {avg_time:.2f}s")
            
            # Log or record what was changed, if you like
            logger.info(f"Corrected analysis by postprocess: {analysis}")
            
            return AudioAnalysisResponse(
                transcription=transcription,
                **analysis
            )
            
        except Exception as e:
            self.metrics["failed_requests"] += 1
            logger.error(f"Processing error: {e}")
            raise HTTPException(status_code=500, detail=str(e))
        finally:
            if temp_file_path and os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
    
    async def _transcribe_audio(self, audio_file_path: str) -> str:
        audio = AudioSegment.from_file(audio_file_path)
        audio_length_ms = len(audio)
        if audio_length_ms <= 30000:
            result = self.asr_pipeline(audio_file_path)
            return result["text"].strip()
        
        chunk_length_ms = 27000
        stride_length_ms = 2000
        chunks = []
        for start_ms in range(0, audio_length_ms, chunk_length_ms - stride_length_ms):
            end_ms = min(start_ms + chunk_length_ms, audio_length_ms)
            chunk = audio[start_ms:end_ms]
            tmp_file = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
            chunk.export(tmp_file.name, format="wav", parameters=["-ar", "16000"])
            chunks.append(tmp_file.name)
            tmp_file.close()
        
        def transcribe_chunk(chunk_path):
            try:
                result = self.asr_pipeline(chunk_path)
                return result["text"].strip()
            finally:
                os.unlink(chunk_path)

        with concurrent.futures.ThreadPoolExecutor() as executor:
            texts = list(executor.map(transcribe_chunk, chunks))
        return " ".join(texts)
    
    async def _analyze_with_llm(self, transcription: str) -> Dict[str, bool]:
        """Analyze transcription using vLLM"""
        try:
            # Skip very short transcriptions
            if len(transcription) < 10 or transcription == "transcription_error":
                return self._get_default_analysis()
            
            # Truncate long transcriptions
            if len(transcription) > 4000:
                transcription = transcription[:4000] + "..."
            
            # Prepare prompt
            messages = [
                {"role": "system", "content": self.system_message},
                {"role": "user", "content": f"à¸šà¸—à¸ªà¸™à¸—à¸™à¸²: {transcription}"}
            ]
            
            prompt = self.tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=False,
            )
            
            # vLLM sampling parameters
            sampling_params = SamplingParams(
                temperature=1.0,  # Deterministic
                max_tokens=200,
                stop_token_ids=[self.tokenizer.eos_token_id] if self.tokenizer.eos_token_id else None,
            )
            
            # Generate with vLLM
            outputs = self.llm.generate([prompt], sampling_params, use_tqdm=False)
            response_text = outputs[0].outputs[0].text
            # print(f"===> Debug responsetxt: {response_text}")
            
            # Extract JSON from response
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            if json_start != -1 and json_end > json_start:
                json_str = response_text[json_start:json_end]
                result = json.loads(json_str)
                
                return {
                    "is_greeting": bool(result.get('à¸à¸¥à¹ˆà¸²à¸§à¸ªà¸§à¸±à¸ªà¸”à¸µ', 0)),
                    "is_introself": bool(result.get('à¹à¸™à¸°à¸™à¸³à¸Šà¸·à¹ˆà¸­à¹à¸¥à¸°à¸™à¸²à¸¡à¸ªà¸à¸¸à¸¥', 0)),
                    "is_informlicense": bool(result.get('à¸šà¸­à¸à¸›à¸£à¸°à¹€à¸ à¸—à¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¹à¸¥à¸°à¹€à¸¥à¸‚à¸—à¸µà¹ˆà¹ƒà¸šà¸­à¸™à¸¸à¸à¸²à¸•à¸—à¸µà¹ˆà¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸«à¸¡à¸”à¸­à¸²à¸¢à¸¸', 0)),
                    "is_informobjective": bool(result.get('à¸šà¸­à¸à¸§à¸±à¸•à¸–à¸¸à¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œà¸‚à¸­à¸‡à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸žà¸šà¸„à¸£à¸±à¹‰à¸‡à¸™à¸µà¹‰', 0)),
                    "is_informbenefit": bool(result.get('à¹€à¸™à¹‰à¸™à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œà¸§à¹ˆà¸²à¸¥à¸¹à¸à¸„à¹‰à¸²à¹„à¸”à¹‰à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œà¸­à¸°à¹„à¸£à¸ˆà¸²à¸à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸žà¸šà¸„à¸£à¸±à¹‰à¸‡à¸™à¸µà¹‰', 0)),
                    "is_informinterval": bool(result.get('à¸šà¸­à¸à¸£à¸°à¸¢à¸°à¹€à¸§à¸¥à¸²à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¹ƒà¸™à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸žà¸š', 0)),
                }
            else:
                logger.error(f"Failed to parse JSON from response: {response_text}")
                return self._get_default_analysis()
                
        except Exception as e:
            logger.error(f"LLM analysis error: {e}")
            return self._get_default_analysis()
    
    def _get_default_analysis(self) -> Dict[str, bool]:
        """Return default analysis (all False)"""
        return {
            "is_greeting": False,
            "is_introself": False,
            "is_informlicense": False,
            "is_informobjective": False,
            "is_informbenefit": False,
            "is_informinterval": False,
        }

# Additional endpoints for monitoring
@serve.deployment
class HealthCheck:
    async def __call__(self):
        return {
            "status": "healthy",
            "ray_cluster": ray.cluster_resources(),
            "timestamp": str(asyncio.get_event_loop().time())
        }

# Deploy the services
deployment = ASRLLMService.bind()
health_deployment = HealthCheck.bind()

# Run both deployments with route prefixes
serve.run(deployment, name="asr_llm_service", route_prefix="/eval")
serve.run(health_deployment, name="health_check", route_prefix="/health")

if __name__ == "__main__":
    # Keep the script running
    import time
    logger.info("ASR-LLM Service with vLLM is running...")
    logger.info("Endpoints:")
    logger.info("  - POST http://0.0.0.0:4000/eval (main service)")
    logger.info("  - GET  http://0.0.0.0:4000/health (health check)")
    
    try:
        while True:
            time.sleep(60)
            # Optional: Add periodic health checks or metrics logging
    except KeyboardInterrupt:
        logger.info("Shutting down service...")
        serve.shutdown()